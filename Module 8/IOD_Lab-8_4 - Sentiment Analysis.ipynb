{"cells":[{"cell_type":"markdown","metadata":{"id":"UYvQOebqLcfM"},"source":["<div>\n","<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"1JWvLBewLcfP"},"source":["# Lab 8.4: Sentiment Analysis\n","INSTRUCTIONS:\n","- Run the cells\n","- Observe and understand the results\n","- Answer the questions"]},{"cell_type":"markdown","metadata":{"id":"HbhmKC6NLcfS"},"source":["Based on the video tutorial **Text Classification with Machine Learning,SpaCy and Scikit(Sentiment Analysis)** by **Jesse E. Agbe (JCharis)**."]},{"cell_type":"markdown","metadata":{"id":"NnuAMgbhLcfV"},"source":["## Data Source: UCI\n","### UCI - Machine Learning Repository\n","- Center for Machine Learning and Intelligent Systems\n","\n","The [**UCI Machine Learning Repository**](http://archive.ics.uci.edu/about) is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms.\n","\n","### Dataset\n","- [Sentiment Labelled Sentences Data Set](http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)\n","\n","### Abstract\n","The dataset contains sentences labelled with positive or negative sentiment.\n","\n","- Data Set Characteristics: Text\n","- Number of Instances: 3000\n","- Area: N/A\n","- Attribute Characteristics: N/A\n","- Number of Attributes: N/A\n","- Date Donated: 2015-05-30\n","- Associated Tasks: Classification\n","- Missing Values? N/A\n","- Number of Web Hits: 102584\n","\n","### Source\n","Dimitrios Kotzias dkotzias '@' ics.uci.edu\n","\n","### Data Set Information\n","This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\n","\n","Please cite the paper if you want to use it :)\n","\n","It contains sentences labelled with positive or negative sentiment.\n","\n","### Format\n","sentence &lt;tab&gt; score &lt;newline&gt;\n","\n","### Details\n","Score is either 1 (for positive) or 0 (for negative)\n","\n","The sentences come from three different websites/fields:\n","- imdb.com\n","- amazon.com\n","- yelp.com\n","\n","For each website, there exist **500 positive** and **500 negative** sentences. Those were selected randomly for larger datasets of reviews.\n","\n","We attempted to select sentences that have a clearly positive or negative connotation, the goal was for no neutral sentences to be selected.\n","\n","For the full datasets look:\n","\n","- **imdb**: Maas et. al., 2011 _Learning word vectors for sentiment analysis_\n","- **amazon**: McAuley et. al., 2013 _Hidden factors and hidden topics: Understanding rating dimensions with review text_\n","- **yelp**: [Yelp dataset challenge](http://www.yelp.com/dataset_challenge)\n","\n","\n","### Attribute Information\n","The attributes are text sentences, extracted from reviews of products, movies, and restaurants\n","\n","### Relevant Papers\n","**From Group to Individual Labels using Deep Features**, Kotzias et. al,. KDD 2015\n","\n","### Citation Request\n","**From Group to Individual Labels using Deep Features**, Kotzias et. al,. KDD 2015"]},{"cell_type":"markdown","metadata":{"id":"abNvVWdlLcfW"},"source":["## Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:27:26.865620Z","start_time":"2019-06-17T01:27:24.368522Z"},"id":"4BJWjM0zLcfZ"},"outputs":[],"source":["## Import Libraries\n","import pandas as pd\n","\n","import regex as re\n","import spacy\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import seaborn as sns\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.svm import SVC\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"Dzzk6JdcLcfh"},"source":["## Load data\n","\n","Load Yelp, Amazon and Imdb Data.\n","\n","Hint: Source is separated by <tab>s and has no headers."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:29:38.157718Z","start_time":"2019-06-17T01:29:38.152747Z"},"id":"GZUWhcCuLcfi"},"outputs":[],"source":["yelp_text = 'yelp_labelled.txt'\n","imdb_text = 'imdb_labelled_fixed.txt'\n","amazon_text = 'amazon_cells_labelled.txt'\n","\n","# ANSWER"]},{"cell_type":"markdown","metadata":{"id":"pwa3MBrwLcfo"},"source":["## Inspect the data\n","\n","Check your datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:30:01.495935Z","start_time":"2019-06-17T01:30:01.492941Z"},"id":"NddGh-EQLcfq"},"outputs":[],"source":["# ANSWER"]},{"cell_type":"markdown","metadata":{"id":"meEtfGfELcf4"},"source":["## Merge the data\n","\n","Merge all three datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:30:37.302897Z","start_time":"2019-06-17T01:30:37.299903Z"},"id":"WVpAx-HHcbwn"},"outputs":[],"source":["# ANSWER"]},{"cell_type":"markdown","metadata":{"id":"QBIFtbMALcf8"},"source":["## Export the data\n","\n","Export merged datasets to as csv file."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:31:16.041727Z","start_time":"2019-06-17T01:31:16.037738Z"},"id":"n8OLkaALLcf9","scrolled":false},"outputs":[],"source":["# ANSWER"]},{"cell_type":"markdown","metadata":{"id":"bzA4FQsPLcgA"},"source":["## Prepare the stage\n","- Load spaCy"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:31:19.686599Z","start_time":"2019-06-17T01:31:18.952239Z"},"id":"wVMTSDYQLcgB"},"outputs":[],"source":["nlp = spacy.load('en')"]},{"cell_type":"markdown","metadata":{"id":"YguMrtDuLcgD"},"source":["## Prepare the text\n","All the text handling and preparation concerned with the changes and modifications from the raw source text to a format that will be used for the actual processing, things like:\n","- handle encoding\n","- handle extraneous and international characters\n","- handle symbols\n","- handle metadata and embedded information\n","- handle repetitions (such multiple spaces or newlines)\n","\n","Clean text."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:31:31.608285Z","start_time":"2019-06-17T01:31:31.601306Z"},"id":"GlsKSvonLcgD","scrolled":true},"outputs":[],"source":["def clean_text(text):\n","    # reduce multiple spaces and newlines to only one\n","    text = re.sub(r'(\\s\\s+|\\n\\n+)', r'\\1', text)\n","    # remove double quotes\n","    text = re.sub(r'\"', '', text)\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:32:56.768268Z","start_time":"2019-06-17T01:32:56.765283Z"},"id":"upPa3YmmLcgF"},"outputs":[],"source":["# Apply the clean_text function to your dataset.\n","# ANSWER"]},{"cell_type":"markdown","metadata":{"id":"za_6vt3MLcgH"},"source":["## Work the text\n","Using techniques learned in previous labs, remove StopWords, punctuation, and digits. Entities can be retained. Return the lemmatised form of any remaining words in lower case form.\n","\n","This removes meaningless information."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:32:58.911623Z","start_time":"2019-06-17T01:32:58.897659Z"},"id":"sh_uDWcCLcgI"},"outputs":[],"source":["# Complete the function\n","def convert_text(text):\n","    '''\n","    Use techniques learned in previous labs.\n","    1) Remove StopWords, Punctuation and digits.\n","    2) Retain entities.\n","    3) Return the lemmatised form of remaining words in lower case form.\n","    '''\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:33:42.014624Z","start_time":"2019-06-17T01:33:01.620538Z"},"id":"0vDv55U1LcgK","outputId":"6ae31463-3509-4ea6-934a-6ea1ff1f6b6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wall time: 40.4 s\n"]}],"source":["%%time\n","df['short'] = df['text'].apply(convert_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:13.381487Z","start_time":"2019-06-17T01:35:13.362526Z"},"id":"faiuJfunLcgM","outputId":"67d67ec3-44c6-4315-ef42-7467f69494d5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","      <th>short</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1989</th>\n","      <td>:) Anyway, the plot flowed smoothly and the ma...</td>\n","      <td>1</td>\n","      <td>anyway plot flow smoothly male bond scene hoot</td>\n","    </tr>\n","    <tr>\n","      <th>2677</th>\n","      <td>Truly awful.</td>\n","      <td>0</td>\n","      <td>truly awful</td>\n","    </tr>\n","    <tr>\n","      <th>650</th>\n","      <td>Very Very Disappointed ordered the $35 Big Bay...</td>\n","      <td>0</td>\n","      <td>very very disappointed order $ 35 big bay plater</td>\n","    </tr>\n","    <tr>\n","      <th>1157</th>\n","      <td>I had to walk out of the theatre for a few min...</td>\n","      <td>0</td>\n","      <td>-pron- walk theatre minute relief</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>Unfortunately, any virtue in this film's produ...</td>\n","      <td>0</td>\n","      <td>unfortunately virtue film 's production work l...</td>\n","    </tr>\n","    <tr>\n","      <th>1507</th>\n","      <td>By the time the film ended, I not only dislike...</td>\n","      <td>0</td>\n","      <td>by time film end -pron- dislike -pron- despise</td>\n","    </tr>\n","    <tr>\n","      <th>1721</th>\n","      <td>So mediocre in every aspect that it just becom...</td>\n","      <td>0</td>\n","      <td>so mediocre aspect dull uninteresting mess for...</td>\n","    </tr>\n","    <tr>\n","      <th>223</th>\n","      <td>Hopefully this bodes for them going out of bus...</td>\n","      <td>0</td>\n","      <td>hopefully bode go business cook come</td>\n","    </tr>\n","    <tr>\n","      <th>1870</th>\n","      <td>Either way, it sucks.</td>\n","      <td>0</td>\n","      <td>either way suck</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>AN HOUR... seriously?</td>\n","      <td>0</td>\n","      <td>an hour seriously</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   text  sentiment  \\\n","1989  :) Anyway, the plot flowed smoothly and the ma...          1   \n","2677                                       Truly awful.          0   \n","650   Very Very Disappointed ordered the $35 Big Bay...          0   \n","1157  I had to walk out of the theatre for a few min...          0   \n","1996  Unfortunately, any virtue in this film's produ...          0   \n","1507  By the time the film ended, I not only dislike...          0   \n","1721  So mediocre in every aspect that it just becom...          0   \n","223   Hopefully this bodes for them going out of bus...          0   \n","1870                            Either way, it sucks.            0   \n","568                               AN HOUR... seriously?          0   \n","\n","                                                  short  \n","1989   anyway plot flow smoothly male bond scene hoot    \n","2677                                        truly awful  \n","650    very very disappointed order $ 35 big bay plater  \n","1157                -pron- walk theatre minute relief    \n","1996  unfortunately virtue film 's production work l...  \n","1507   by time film end -pron- dislike -pron- despise    \n","1721  so mediocre aspect dull uninteresting mess for...  \n","223                hopefully bode go business cook come  \n","1870                                  either way suck    \n","568                                   an hour seriously  "]},"execution_count":23,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["df.sample(10)"]},{"cell_type":"markdown","metadata":{"id":"TbwjijVyLcgP"},"source":["## Split the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:24.658233Z","start_time":"2019-06-17T01:35:24.649227Z"},"id":"Hj2aoBqqLcgV"},"outputs":[],"source":["# Features and Labels\n","X = df['short']\n","y = df['sentiment']\n","\n","# Apply a train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"]},{"cell_type":"markdown","metadata":{"id":"yr_VmeNMLcgY"},"source":["## Create a Bag-of-Words Model"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:32.373670Z","start_time":"2019-06-17T01:35:32.369681Z"},"id":"Rhd__LD6LcgZ"},"outputs":[],"source":["# create a matrix of word counts from the text\n","counts = CountVectorizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:35.842101Z","start_time":"2019-06-17T01:35:35.784219Z"},"id":"23CpVgPxLcgb"},"outputs":[],"source":["# do the actual counting\n","A = counts.fit_transform(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:38.590493Z","start_time":"2019-06-17T01:35:38.586469Z"},"id":"c_rue57RLcgd"},"outputs":[],"source":["# create a classifier using SVC\n","classifier = SVC(kernel='linear', probability=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:41.929126Z","start_time":"2019-06-17T01:35:41.745617Z"},"id":"Lou4xDLmLcgh","outputId":"cbc743d3-09c6-448c-bd75-75f9d5366211"},"outputs":[{"data":{"text/plain":["LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n","     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n","     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n","     verbose=0)"]},"execution_count":29,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# train the classifier with the training data\n","classifier.fit(A.toarray(), y_train)"]},{"cell_type":"code","source":[],"metadata":{"id":"8O_Vkm4tc6jx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:47.210207Z","start_time":"2019-06-17T01:35:47.199250Z"},"id":"inkg1KTiLcgi"},"outputs":[],"source":["# do the transformation for the test data\n","# NOTE: use `transform()` instead of `fit_transform()`\n","B = counts.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:51.223067Z","start_time":"2019-06-17T01:35:51.209096Z"},"id":"dg-HpdJ0Lcgk"},"outputs":[],"source":["# make predictions based on the test data\n","predictions = classifier.predict(B.todense())\n","\n","# store probabilities of predictions being 1\n","probabilities = classifier.predict_proba(B.todense())[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:54.779047Z","start_time":"2019-06-17T01:35:54.771069Z"},"id":"t0HJn9qhLcgm","outputId":"0bc7328f-ed1e-4259-e02f-981413ab8bc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7833\n"]}],"source":["# check the accuracy\n","print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"]},{"cell_type":"markdown","metadata":{"id":"z-Ia6a8ULcgn"},"source":["## Repeat using TF-IDF\n","TF-IDF = Term Frequency - Inverse Document Frequency"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:36:02.927008Z","start_time":"2019-06-17T01:36:02.785387Z"},"id":"7Tg1dwSpLcgo","outputId":"256d6cbb-663b-4f6d-daa6-c609c9ec18ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7983\n"]}],"source":["# create a matrix of word counts from the text\n","# use TF-IDF\n","tfidf = TfidfVectorizer()\n","# do the actual counting\n","A = tfidf.fit_transform(X_train, y_train)\n","\n","# train the classifier with the training data\n","classifier.fit(A.toarray(), y_train)\n","\n","# do the transformation for the test data\n","# NOTE: use `transform()` instead of `fit_transform()`\n","B = tfidf.transform(X_test)\n","\n","# make predictions based on the test data\n","predictions = classifier.predict(B)\n","\n","# store probabilities of predictions being 1\n","probabilities = classifier.predict_proba(B.todense())[:, 1]\n","\n","# check the accuracy\n","print('Accuracy: %.4f' % accuracy_score(y_test, predictions))"]},{"cell_type":"markdown","source":["## Defining a helper function to show results and charts"],"metadata":{"id":"CXbw_oNdZAHv"}},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:35:22.212854Z","start_time":"2019-06-17T01:35:22.040284Z"},"id":"eJZpD903LcgQ"},"outputs":[],"source":["\n","def show_summary_report(actual, prediction, probabilities):\n","\n","    if isinstance(actual, pd.Series):\n","        actual = actual.values.astype(int)\n","    prediction = prediction.astype(int)\n","\n","    accuracy_ = accuracy_score(actual, prediction)\n","    precision_ = precision_score(actual, prediction)\n","    recall_ = recall_score(actual, prediction)\n","    roc_auc_ = roc_auc_score(actual, probabilities)\n","\n","    print('Accuracy : %.4f [TP / N] Proportion of predicted labels that match the true labels. Best: 1, Worst: 0' % accuracy_)\n","    print('Precision: %.4f [TP / (TP + FP)] Not to label a negative sample as positive.        Best: 1, Worst: 0' % precision_)\n","    print('Recall   : %.4f [TP / (TP + FN)] Find all the positive samples.                     Best: 1, Worst: 0' % recall_)\n","    print('ROC AUC  : %.4f                                                                     Best: 1, Worst: < 0.5' % roc_auc_)\n","    print('-' * 107)\n","    print('TP: True Positives, FP: False Positives, TN: True Negatives, FN: False Negatives, N: Number of samples')\n","\n","    # Confusion Matrix\n","    mat = confusion_matrix(actual, prediction)\n","\n","    # Precision/Recall\n","    precision, recall, _ = precision_recall_curve(actual, probabilities)\n","    average_precision = average_precision_score(actual, probabilities)\n","\n","    # Compute ROC curve and ROC area\n","    fpr, tpr, _ = roc_curve(actual, probabilities)\n","    roc_auc = auc(fpr, tpr)\n","\n","\n","    # plot\n","    fig, ax = plt.subplots(1, 3, figsize = (18, 6))\n","    fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n","\n","    # Confusion Matrix\n","    sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = False, cmap = 'Blues', ax = ax[0])\n","\n","    ax[0].set_title('Confusion Matrix')\n","    ax[0].set_xlabel('True label')\n","    ax[0].set_ylabel('Predicted label')\n","\n","    # Precision/Recall\n","    step_kwargs = {'step': 'post'}\n","    ax[1].step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n","    ax[1].fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n","    ax[1].set_ylim([0.0, 1.0])\n","    ax[1].set_xlim([0.0, 1.0])\n","    ax[1].set_xlabel('Recall')\n","    ax[1].set_ylabel('Precision')\n","    ax[1].set_title('2-class Precision-Recall curve')\n","\n","    # ROC\n","    ax[2].plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = %0.2f)' % roc_auc)\n","    ax[2].plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n","    ax[2].set_xlim([0.0, 1.0])\n","    ax[2].set_ylim([0.0, 1.0])\n","    ax[2].set_xlabel('False Positive Rate')\n","    ax[2].set_ylabel('True Positive Rate')\n","    ax[2].set_title('Receiver Operating Characteristic')\n","    ax[2].legend(loc = 'lower right')\n","\n","    plt.show()\n","\n","    return (accuracy_, precision_, recall_, roc_auc_)"]},{"cell_type":"markdown","metadata":{"id":"O5PTu402Lcgq"},"source":["## Repeating it all for comparision\n","Repeating the whole lot in one big block using the show_summary_report function.\n","\n","Find 'Accuracy', 'Precision', 'Recall', 'ROC_AUC' using CountVectorizer and TfidfVectorizer and keep the result in a dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-06-17T01:37:30.200048Z","start_time":"2019-06-17T01:37:30.197044Z"},"id":"_98CzdfPLcgq"},"outputs":[],"source":["# ANSWER"]},{"cell_type":"markdown","metadata":{"id":"RERADKgNFq9T"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","> > > > > > > > > Â© 2024 Institute of Data\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}